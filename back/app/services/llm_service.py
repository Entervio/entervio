"""LLM Service using Google Gemini for Interview Scenarios"""
import google.generativeai as genai
from typing import List, Dict, Literal, Any
import numpy as np
import logging
from app.core.config import settings
import json

# Setup logging
logger = logging.getLogger(__name__)

InterviewerType = Literal["nice", "neutral", "mean"]

# Base interview instructions (common to all types)
BASE_INTERVIEW_INSTRUCTIONS = """Tu es un recruteur professionnel fran√ßais qui m√®ne un entretien d'embauche.

R√àGLES CRITIQUES - FEEDBACK CONCIS:
- Donne des feedbacks TR√àS COURTS (1-2 phrases maximum)
- NE PAS √©crire de longs paragraphes de f√©licitations
- NE PAS dire "c'est excellent", "vous √™tes g√©nial", "parfait" √† r√©p√©tition
- Feedback format: "Bien." ou "Int√©ressant." puis PASSE √Ä LA QUESTION SUIVANTE
- Exemple: "D'accord, je comprends. Parlons maintenant de..."

STRUCTURE DE L'ENTRETIEN:
- L'entretien doit durer environ 5 questions au total
- Compte mentalement les questions pos√©es
- Apr√®s la 5√®me question, conclus naturellement l'entretien
- Questions: 1) Pr√©sentation, 2) Exp√©rience cl√©, 3) Comp√©tences techniques, 4) Motivations, 5) Question de situation/d√©fi

STYLE DE QUESTIONS:
- Questions directes et professionnelles
- Pas de questions trop longues
- √âcoute les r√©ponses et adapte-toi
- Pose des questions de suivi si n√©cessaire mais reste dans la limite de 5 questions totales"""

# Interviewer personality prompts
INTERVIEWER_PROMPTS = {
    "nice": """PERSONNALIT√â: Recruteur Bienveillant et Encourageant

Tu es chaleureux, positif et encourageant. Tu mets le candidat √† l'aise.

COMPORTEMENT:
- Ton accueillant et amical
- Souris dans ta voix (utilise un langage positif)
- Encourage le candidat: "C'est tr√®s bien", "J'aime votre approche"
- Feedbacks positifs mais COURTS: "Super." puis question suivante
- Cr√©e une atmosph√®re d√©tendue et confortable
- Reformule positivement: "Int√©ressant, et si on parlait de..."

EXEMPLE DE STYLE:
‚ùå MAUVAIS: "Wow, c'est absolument fantastique ! Votre exp√©rience est vraiment impressionnante et montre une grande maturit√© professionnelle. Je suis vraiment ravi d'entendre cela !"
‚úÖ BON: "Tr√®s bien, j'appr√©cie votre franchise. Maintenant, parlez-moi d'un projet technique..."

IMPORTANT: Reste bienveillant mais CONCIS dans tes feedbacks.""",

    "neutral": """PERSONNALIT√â: Recruteur Professionnel et Objectif

Tu es neutre, factuel et professionnel. Tu √©values objectivement sans √™tre ni trop chaleureux ni froid.

COMPORTEMENT:
- Ton professionnel et mesur√©
- Feedbacks factuels et COURTS: "D'accord." puis question suivante
- Pas d'√©motions excessives (ni trop positif ni n√©gatif)
- Questions directes et claires
- √âcoute attentive mais sans commentaires √©labor√©s
- Transitions neutres: "Je vois. Passons √†...", "Compris. Maintenant..."

EXEMPLE DE STYLE:
‚ùå MAUVAIS: "Merci pour cette r√©ponse d√©taill√©e. C'est effectivement une approche int√©ressante qui d√©montre votre capacit√© d'analyse."
‚úÖ BON: "D'accord. Parlez-moi d'une situation difficile que vous avez g√©r√©e."

IMPORTANT: Reste neutre et CONCIS dans tes feedbacks.""",

    "mean": """PERSONNALIT√â: Recruteur Exigeant et Direct

Tu es exigeant, critique et direct. Tu testes la r√©sistance au stress du candidat.

COMPORTEMENT:
- Ton sec et direct, parfois l√©g√®rement sarcastique
- Feedbacks critiques mais COURTS: "Hmm." ou "On verra." puis question suivante
- Questions qui challengent le candidat
- Rel√®ve les faiblesses: "C'est tout ?", "Plut√¥t banal."
- Cr√©e une l√©g√®re pression (reste professionnel, pas insultant)
- Scepticisme dans les transitions: "Bien, et concr√®tement...", "Passons √† autre chose."

EXEMPLE DE STYLE:
‚ùå MAUVAIS: "Votre r√©ponse manque vraiment de substance et je dois dire que je m'attendais √† beaucoup mieux de la part d'un candidat avec votre profil."
‚úÖ BON: "Hmm, c'est vague. Donnez-moi un exemple concret avec des r√©sultats chiffr√©s."

IMPORTANT: Sois exigeant mais garde des feedbacks COURTS. Ne sois pas m√©chant, juste direct et exigeant."""
}

def get_system_prompt(interviewer_type: InterviewerType, candidate_context: str = "", job_description: str = "") -> str:
    """Get the complete system prompt for the given interviewer type."""
    base_prompt = f"{BASE_INTERVIEW_INSTRUCTIONS}\n\n{INTERVIEWER_PROMPTS[interviewer_type]}"
    
    if job_description:
        base_prompt += f"\n\nDESCRIPTION DU POSTE:\n{job_description}\n\nINSTRUCTION: Tu dois mener cet entretien sp√©cifiquement pour ce poste. Tes questions doivent √©valuer l'ad√©quation du candidat avec cette description."

    if candidate_context:
        base_prompt += f"\n\nCONTEXTE DU CANDIDAT (CV):\n{candidate_context}\n\nINSTRUCTION: Utilise ce contexte pour poser des questions personnalis√©es sur l'exp√©rience et les comp√©tences du candidat."
    return base_prompt


class LLMService:
    def __init__(self):
        """Initialize with Google Gemini using settings from config."""
        logger.info("üîÑ Initializing LLMService...")
        
        # Get API key from settings
        api_key = settings.GEMINI_API_KEY
        
        if not api_key:
            logger.warning(
                "‚ö†Ô∏è  GEMINI_API_KEY not configured. "
                "LLM features will not work. Add GEMINI_API_KEY to .env"
            )
            self.api_key = None
            self.client_ready = False
            return
        
        logger.info(f"‚úì Found GEMINI_API_KEY: {api_key[:10]}...{api_key[-5:]}")
        
        try:
            genai.configure(api_key=api_key)
            self.api_key = api_key
            self.client_ready = True
            # Note: Model will be created per-session with appropriate system prompt
            logger.info("‚úÖ Gemini client initialized successfully!")
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize Gemini client: {str(e)}")
            self.client_ready = False
            self.api_key = None
    
    def _create_model(self, interviewer_type: InterviewerType, candidate_context: str = "", job_description: str = ""):
        """Create a Gemini model with the appropriate system prompt."""
        if not self.client_ready:
            raise ValueError("Gemini client not initialized. Please set GEMINI_API_KEY in .env")
        system_prompt = get_system_prompt(interviewer_type, candidate_context, job_description)
        return genai.GenerativeModel(
            'gemini-2.0-flash-lite-preview-02-05',
            system_instruction=system_prompt
        )

    def _create_grading_model(self, interviewer_type: InterviewerType):
        """Create a gemini model to grade the user responses"""
        system_prompt = get_system_prompt(interviewer_type)
        return genai.GenerativeModel(
            'gemini-2.0-flash-lite-preview-02-05',
            system_instruction=system_prompt,
            generation_config={
                "response_mime_type": "application/json"
            }
        )
    
    def get_initial_greeting(
        self, 
        candidate_name: str, 
        interviewer_type: InterviewerType,
        candidate_context: str = "",
        job_description: str = ""
    ) -> str:
        """
        Generate personalized initial greeting based on interviewer type.
        
        Args:
            candidate_name: The candidate's name
            interviewer_type: Type of interviewer (nice, neutral, mean)
            candidate_context: Context from resume
            job_description: Job description context
            
        Returns:
            Personalized greeting message
        """
        logger.info(f"üëã Generating greeting for {candidate_name} with {interviewer_type} interviewer")
        
        greetings = {
            "nice": f"""Bonjour {candidate_name} ! Je suis absolument ravi de vous rencontrer aujourd'hui. 

Je serai votre interlocuteur pour cet entretien et je veux que vous vous sentiez parfaitement √† l'aise. Mon objectif est de d√©couvrir qui vous √™tes vraiment, vos talents et vos aspirations.

N'h√©sitez surtout pas √† √™tre vous-m√™me - il n'y a pas de mauvaises r√©ponses ici ! Je suis simplement curieux d'en apprendre plus sur vous.

Pour commencer, pourriez-vous vous pr√©senter en quelques mots ? Parlez-moi de votre parcours.""",

            "neutral": f"""Bonjour {candidate_name}. 

Je serai votre interlocuteur aujourd'hui. L'objectif de cet entretien est d'√©valuer votre profil, vos comp√©tences et votre ad√©quation avec le poste.

Nous allons passer en revue votre exp√©rience et vos motivations. Soyez pr√©cis dans vos r√©ponses.

Commen√ßons. Pr√©sentez-vous bri√®vement.""",

            "mean": f"""Bonjour {candidate_name}.

Je n'ai pas beaucoup de temps, alors allons droit au but. J'ai vu beaucoup de candidats cette semaine et franchement, peu m'ont impressionn√©.

J'attends des r√©ponses concr√®tes, avec des exemples pr√©cis et des r√©sultats mesurables. Pas de langue de bois.

Pr√©sentez-vous. Et soyez synth√©tique."""
        }
        
        return greetings[interviewer_type]
    
    async def chat(
        self, 
        message: str, 
        conversation_history: List[Dict[str, str]],
        interviewer_type: InterviewerType,
        candidate_context: str = "",
        job_description: str = ""
    ) -> str:
        """
        Send message to Gemini and get interviewer response.
        
        Args:
            message: Candidate's message
            conversation_history: List of previous messages
            interviewer_type: Type of interviewer
            candidate_context: Context from resume
            job_description: Job description context
            
        Returns:
            Interviewer's response text
        """
        logger.info(f"üí¨ Processing candidate response with {interviewer_type} interviewer")
        
        try:
            # Create model with appropriate personality and context
            model = self._create_model(interviewer_type, candidate_context, job_description)
            
            # Convert conversation history to Gemini format
            history = []
            if conversation_history:
                for msg in conversation_history:
                    role = "model" if msg["role"] == "assistant" else msg["role"]
                    history.append({
                        "role": role,
                        "parts": [msg["content"]]
                    })
            
            # Start chat with history
            chat = model.start_chat(history=history)
            
            # Send message and get response
            response = chat.send_message(message)
            response_text = response.text
            
            logger.info(f"‚úÖ Got {interviewer_type} interviewer response ({len(response_text)} chars)")
            return response_text
            
        except Exception as e:
            logger.error(f"‚ùå Chat error: {str(e)}")
            raise
    
    async def grade_response(
    self,
    question: str,
    answer: str,
    interviewer_type: InterviewerType
    ) -> Dict[str, any]:
        """
        Grade a candidate's response to an interview question.
        
        Args:
            question: The interview question asked
            answer: The candidate's answer
            interviewer_type: Type of interviewer (affects grading strictness)
            
        Returns:
            Dict with 'grade' (1-10) and 'feedback' (str)
        """
        logger.info(f"üìä Grading response with {interviewer_type} interviewer...")
        
        try:
            # Create grading model with JSON output
            model = self._create_grading_model(interviewer_type)
            
            # Grading prompt with strict JSON schema
            grading_prompt = f"""Tu dois √©valuer la r√©ponse d'un candidat √† une question d'entretien.

                                QUESTION POS√âE:
                                {question}

                                R√âPONSE DU CANDIDAT:
                                {answer}

                                CONSIGNES D'√âVALUATION:
                                - Note de 1 √† 10 (1 = tr√®s mauvais, 10 = excellent)
                                - Feedback concis en fran√ßais (2-3 phrases maximum)
                                - √âvalue: pertinence, clart√©, exemples concrets, structure

                                R√©ponds UNIQUEMENT avec ce format JSON exact:
                                {{
                                "grade": 8,
                                "feedback": "R√©ponse claire avec un bon exemple. Manque de chiffres pr√©cis."
                                }}"""

            # Generate response
            response = model.generate_content(grading_prompt)
            
            # Parse JSON response
            result = json.loads(response.text)
            
            logger.info(f"‚úÖ Response graded: {result['grade']}/10")
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå Failed to parse JSON response: {str(e)}")
            logger.error(f"Raw response: {response.text}")
            # Fallback response
            return {
                "grade": 5,
                "feedback": "Erreur lors de l'√©valuation de la r√©ponse."
            }
        except Exception as e:
            logger.error(f"‚ùå Grading error: {str(e)}")
            raise

    async def end_interview(
        self, 
        conversation_history: List[Dict[str, str]],
        interviewer_type: InterviewerType
    ) -> str:
        """
        Generate feedback and analysis for the interview.
        
        Args:
            conversation_history: Full conversation history
            interviewer_type: Type of interviewer
            
        Returns:
            Structured feedback and analysis of the interview performance
        """
        logger.info(f"üìù Generating interview feedback with {interviewer_type} interviewer...")
        
        try:
            model = self._create_model(interviewer_type)
            
            history = []
            for msg in conversation_history:
                role = "model" if msg["role"] == "assistant" else msg["role"]
                history.append({
                    "role": role,
                    "parts": [msg["content"]]
                })
            
            chat = model.start_chat(history=history)
            
            feedback_prompts = {
                "nice": """IMPORTANT: L'entretien est maintenant TERMIN√â. Tu ne poses PLUS de questions.
                
                Ta t√¢che est de r√©diger un FEEDBACK D√âTAILL√â analysant la performance globale du candidat.
                
                Analyse:
                - Les points forts d√©montr√©s durant l'entretien
                - La qualit√© et la pertinence des r√©ponses donn√©es
                - Les exemples concrets fournis
                - Les axes d'am√©lioration possibles
                
                Adopte un ton encourageant et constructif. R√©dige 4-5 phrases en paragraphes.
                Ne pose AUCUNE question. Ne dis pas au revoir. Fournis uniquement l'analyse.""",
                
                "neutral": """IMPORTANT: L'entretien est maintenant TERMIN√â. Tu ne poses PLUS de questions.
                
                Ta t√¢che est de r√©diger un FEEDBACK OBJECTIF analysant la performance du candidat.
                
                Analyse:
                - La structure et la clart√© des r√©ponses
                - La pertinence des exemples et exp√©riences mentionn√©s
                - Les comp√©tences d√©montr√©es
                - Les domaines n√©cessitant un d√©veloppement
                
                Reste factuel et professionnel. R√©dige 4-5 phrases en paragraphes.
                Ne pose AUCUNE question. Ne dis pas au revoir. Fournis uniquement l'analyse.""",
                
                "mean": """IMPORTANT: L'entretien est maintenant TERMIN√â. Tu ne poses PLUS de questions.
                
                Ta t√¢che est de r√©diger un FEEDBACK CRITIQUE analysant la performance du candidat.
                
                Analyse:
                - Les faiblesses identifi√©es dans les r√©ponses
                - Les manques de pr√©paration ou d'exp√©rience concr√®te
                - Les r√©ponses vagues ou insuffisantes
                - Les points √† am√©liorer de mani√®re prioritaire
                
                Sois direct et exigeant dans ton √©valuation. R√©dige 4-5 phrases en paragraphes.
                Ne pose AUCUNE question. Ne dis pas au revoir. Fournis uniquement l'analyse."""
            }
            
            response = chat.send_message(feedback_prompts[interviewer_type])
            feedback = response.text
            
            logger.info("‚úÖ Interview feedback generated")
            return feedback
            
        except Exception as e:
            logger.error(f"‚ùå Error generating feedback: {str(e)}")
            raise

    async def compute_similarity_ranking(
        self,
        candidate_profile: str,
        jobs: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Rerank jobs using Embeddings and Cosine Similarity (RAG approach).
        """
        logger.info(f"‚öñÔ∏è Reranking {len(jobs)} jobs using Embeddings...")
        
        if not jobs:
            return []

        try:
            # 1. Embed Candidate Profile
            # We use the text-embedding-004 model for better performance
            profile_embedding_resp = genai.embed_content(
                model="models/text-embedding-004",
                content=candidate_profile,
                task_type="retrieval_query"
            )
            profile_vector = np.array(profile_embedding_resp['embedding'])

            # 2. Embed Jobs (Batching if necessary, but genai handles lists)
            # Construct texts to embed: "Title: ... Description: ..."
            job_texts = []
            for job in jobs:
                title = job.get("intitule", "")
                desc = job.get("description", "")[:1000] # Truncate for safety
                text = f"Title: {title}\nDescription: {desc}"
                job_texts.append(text)

            # Embed all jobs in one go (or batches of 100 if list is huge)
            # API limit check might be needed for production, but fine for <100 jobs
            jobs_embedding_resp = genai.embed_content(
                model="models/text-embedding-004",
                content=job_texts,
                task_type="retrieval_document"
            )
            
            job_vectors = np.array(jobs_embedding_resp['embedding'])

            # 3. Compute Cosine Similarity
            # Cosine Sim = (A . B) / (||A|| * ||B||)
            # Since embeddings are usually normalized, dot product might suffice, 
            # but let's be mathematically correct.
            
            norm_profile = np.linalg.norm(profile_vector)
            
            reranked_jobs = []
            for i, job in enumerate(jobs):
                job_vector = job_vectors[i]
                norm_job = np.linalg.norm(job_vector)
                
                if norm_profile == 0 or norm_job == 0:
                    similarity = 0.0
                else:
                    similarity = np.dot(profile_vector, job_vector) / (norm_profile * norm_job)
                
                # Normalize score to 0-100 for frontend consistency
                score = int(similarity * 100)
                
                job["relevance_score"] = score
                job["relevance_reasoning"] = "Matched via Vector Similarity"
                reranked_jobs.append(job)

            # 4. Sort
            reranked_jobs.sort(key=lambda x: x["relevance_score"], reverse=True)
            
            logger.info("‚úÖ Jobs reranked via Embeddings")
            return reranked_jobs

        except Exception as e:
            logger.error(f"‚ùå Error in embedding reranking: {str(e)}")
            # Fallback: return original list with 0 score
            for job in jobs:
                job["relevance_score"] = 0
                job["relevance_reasoning"] = "Error in ranking"
            return jobs


# Singleton instance - initialized on first import
_llm_service_instance = None

def get_llm_service() -> LLMService:
    """Get or create the LLM service singleton."""
    global _llm_service_instance
    if _llm_service_instance is None:
        logger.info("üöÄ Creating llm_service singleton...")
        _llm_service_instance = LLMService()
        logger.info("‚úÖ llm_service singleton created!")
    return _llm_service_instance

# For convenience
llm_service = get_llm_service()